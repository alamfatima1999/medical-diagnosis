{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook has embedding size as 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dLQsTfR21jUv",
    "outputId": "43de9d2a-9770-48ea-e45c-c1a93ab24e75"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.config.run_functions_eagerly(True)\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import math\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import keras \n",
    "from keras.backend import max as MAX\n",
    "from keras.layers import Activation, Input, concatenate, dot\n",
    "from keras.layers.core import Dense, Lambda, Reshape\n",
    "from keras.layers.convolutional import Convolution1D\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard, ReduceLROnPlateau\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "from string import punctuation\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "punctuation = list(punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jzWRGvRA1qh0",
    "outputId": "6f5a8de2-2344-4563-ea6c-7c943a4df744"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fasttext in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (0.9.2)\n",
      "Requirement already satisfied: pybind11>=2.2 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from fasttext) (2.10.4)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from fasttext) (47.1.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from fasttext) (1.21.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.1.1; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the 'c:\\users\\lenovo\\appdata\\local\\programs\\python\\python37\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.fasttext import FastText\n",
    "\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "from nltk import ngrams\n",
    "\n",
    "\n",
    "\n",
    "punctuation = list(punctuation)\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import warnings\n",
    " \n",
    "warnings.filterwarnings(action = 'ignore')\n",
    " \n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "\n",
    "#import fasttext as ft\n",
    "#from fasttext import supervised\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "import nltk\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from nltk import WordPunctTokenizer\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "en_stop = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "#get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "\n",
    "from nltk import ngrams\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "punctuation = list(punctuation)\n",
    "\n",
    "!pip install fasttext\n",
    "import fasttext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_mcQ7evI1u5l"
   },
   "outputs": [],
   "source": [
    "#import os\n",
    "#os.environ[\"TF_GPU_ALLOCATOR\"]=\"cuda_malloc_async\"\n",
    "#gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "#for device in gpu_devices:\n",
    "    #tf.config.experimental.set_memory_growth(device, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mjIeh-dh1zcc"
   },
   "outputs": [],
   "source": [
    "#import os\n",
    "#os.environ[\"TF_GPU_ALLOCATOR\"]=\"cuda_malloc_async\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YDGHDp1e13mp",
    "outputId": "d7adfdb3-c2f8-46a2-9ae5-ceb2a5713197"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xjfdT2Wh18Zw",
    "outputId": "265b765f-90d8-4b21-cc7c-0fd20301b175"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Unique ICDs are  11592\n"
     ]
    }
   ],
   "source": [
    "experiment_mode = 'test'\n",
    "if(experiment_mode=='train'):\n",
    "    df = pd.read_csv('C:/Users/Lenovo/Downloads/Project/Data/complaintICD2.csv', encoding = \"ISO-8859-1\",on_bad_lines='skip')\n",
    "    df_neg = pd.read_csv('C:/Users/Lenovo/Downloads/Project/Data/NegativeICDDesc.csv', encoding = \"ISO-8859-1\",on_bad_lines='skip')\n",
    "\n",
    "    #/content/drive/MyDrive/Full_Data.csv\n",
    "\n",
    "    #Read the data\n",
    "    #df=pd.read_csv(data_file,encoding = \"ISO-8859-1\",on_bad_lines='skip')\n",
    "    # df = df.drop(df.columns[-3:], axis=1)\n",
    "\n",
    "    #query with not null values\n",
    "\n",
    "    df.loc[df['SPED_COMLTEXT'].isnull(),'SPED_COMLTEXT'] = \"#\"\n",
    "    df.loc[df['DESCRIPTION'].isnull(),'DESCRIPTION'] = \"#\"\n",
    "\n",
    "\n",
    "    #Get unique values of ICDs\n",
    "    unique_icds = df['SPDD_ICDCODE'].unique()\n",
    "    print(\"Number of Unique ICDs are \",len(unique_icds))\n",
    "    df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "KMv9tWyC0MuE",
    "outputId": "e4db05a0-31bf-4dee-e715-097e70ba1f24"
   },
   "outputs": [],
   "source": [
    "df[df['SPED_COMLTEXT']=='ABD PAIN']\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "Kq_fYcJi2JJm",
    "outputId": "b05bee4b-290a-42a8-bc79-ebe5ec9a2e64"
   },
   "outputs": [],
   "source": [
    "#df_neg.head()\n",
    "#df_neg[['SPED_COMLTEXT'=='ABD PAIN'], ['SPDD_ICDCODE']]\n",
    "df_neg.loc[df_neg['SPED_COMLTEXT']=='ABD PAIN', ['SPED_COMLTEXT', 'SPDD_ICDCODE']]\n",
    "\n",
    "#df_neg[['SPED_COMLTEXT'==self.X_col],['SPDD_ICDCODE', self.y_col]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "0BZjHmJj2ABp"
   },
   "outputs": [],
   "source": [
    "df = df.sort_values(by='SPDD_ICDCODE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b0NpzHau2D8w",
    "outputId": "6b9d8cc8-5699-45fc-ef02-11c8d12b635b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2385894, 7)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "PTpVp43I2IFV"
   },
   "outputs": [],
   "source": [
    "df.loc[df['DESCRIPTION'] == ' unspecified\"', 'DESCRIPTION'] = '#'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "9P9B57J62LxB"
   },
   "outputs": [],
   "source": [
    "df = df.loc[df['DESCRIPTION'] != '#']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "0bzsZWt73wG9"
   },
   "outputs": [],
   "source": [
    "#df.groupby(by='DESCRIPTION').count().sort_values(by='SPDD_ICDCODE')\n",
    "#df = df[12000:13000]\n",
    "\n",
    "experiment_mode = 'test'\n",
    "if(experiment_mode=='test'):\n",
    "  df = pd.read_csv('C:/Users/Lenovo/Downloads/Project/Data/FullTestConcat.csv', encoding = \"ISO-8859-1\",on_bad_lines='skip')\n",
    "  print(df.head())\n",
    "  print(df.shape)\n",
    "  #df=df[5000:10000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "2aLBcw3MbI2l"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2385656, 7)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "mT_1eXi331Os"
   },
   "outputs": [],
   "source": [
    "#Parameters\n",
    "\n",
    "MAX_QUERY_LENGTH = 10\n",
    "MAX_DOCUMENT_LENGTH = 10\n",
    "K = 300 # Dimensionality of the max-pooling layer. See section 3.4.\n",
    "L = 128 # Dimensionality of latent semantic space. See section 3.5.\n",
    "FILTER_LENGTH = 3 # Convolution window size.\n",
    "NEGATIVE_SAMPLE_SIZE = 5\n",
    "TRIGRAM_INDICES = None\n",
    "TOTAL_TRIGRAMS = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "i_1daTf436K5"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stemmer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(document):\n",
    "        # Remove all the special characters\n",
    "        document = re.sub(r'\\W', ' ', str(document))\n",
    "\n",
    "        # remove all single characters\n",
    "        document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "\n",
    "        # Remove single characters from the start\n",
    "        document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document)\n",
    "\n",
    "        # Substituting multiple spaces with single space\n",
    "        document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "\n",
    "        # Removing prefixed 'b'\n",
    "        document = re.sub(r'^b\\s+', '', document)\n",
    "\n",
    "        # Converting to Lowercase\n",
    "        document = document.lower()\n",
    "\n",
    "        # Lemmatization\n",
    "        tokens = document.split()\n",
    "        tokens = [stemmer.lemmatize(word) for word in tokens]\n",
    "        #print(\"initial token ->\")\n",
    "        #print(tokens)\n",
    "        tokens = [word for word in tokens if word not in en_stop]\n",
    "        #print(\"intermediate tokens->\")\n",
    "        #print(tokens)\n",
    "        tokens = [word for word in tokens if len(word) > 3]\n",
    "        #print(\"final tokens ->\")\n",
    "        #print(tokens)\n",
    "\n",
    "        preprocessed_text = ' '.join(tokens)\n",
    "        #print(\"preprocessed_text ->\")\n",
    "        #print(preprocessed_text)\n",
    "\n",
    "        return preprocessed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FN02SLZRBeDI",
    "outputId": "8710be17-7fc5-4566-bab6-839bb58d4e1b",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastText model about to start\n"
     ]
    }
   ],
   "source": [
    "embedding_size = 32\n",
    "#fasttext_model = \"C:/Users/Lenovo/Downloads/Project/FT_model/modelFtUnFull.bin\"\n",
    "if(experiment_mode=='train'):\n",
    "  ft_comp_model = \"C:/Users/Lenovo/Downloads/Project/FT_model/FtFullComp32.bin\"\n",
    "# fasttext_model =\"C:/Users/Lenovo/Downloads/Project/FT_model/cc.en.300.bin\"\n",
    "\n",
    "if(experiment_mode=='train'):\n",
    "    s = \"\"\n",
    "    text = []\n",
    "\n",
    "    for i in df['SPED_COMLTEXT']:\n",
    "        s =\"\" + i + \".\" \n",
    "        text.append(s)\n",
    "    #preprocess text is the funcion specified outside the class\n",
    "    final_corpus = [preprocess_text(sentence) for sentence in text]\n",
    "    word_punctuation_tokenizer = nltk.WordPunctTokenizer()\n",
    "    word_tokenized_corpus = [word_punctuation_tokenizer.tokenize(sent) for sent in final_corpus]\n",
    "    \n",
    "    #embedding_size = 32\n",
    "    #Total unique words: 53983\n",
    "    \"\"\"\n",
    "    unique_words = set([word for sentence in word_tokenized_corpus for word in sentence])\n",
    "    total_unique_words = len(unique_words)\n",
    "    print(\"Total unique words:\", total_unique_words)\n",
    "    \"\"\"\n",
    "\n",
    "    #originally 60\n",
    "    \n",
    "    window_size = 4\n",
    "    min_word = 5\n",
    "    down_sampling = 1e-2\n",
    "    print(\"FastText model about to start\")\n",
    "    ft_model_comp = FastText(word_tokenized_corpus, vector_size = embedding_size, \n",
    "                            window=window_size, min_count = min_word, \n",
    "                            sample=down_sampling, sg=1, epochs=100)\n",
    "    #FastText_model = \n",
    "\n",
    "    ft_model_comp.save(ft_comp_model)\n",
    "    \n",
    "    \n",
    "ft_model_comp = FastText.load(ft_comp_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastText model about to start\n"
     ]
    }
   ],
   "source": [
    "#fasttext_model = \"C:/Users/Lenovo/Downloads/Project/FT_model/modelFtUnFull.bin\"\n",
    "if(experiment_mode=='train'):\n",
    "  ft_desc_model = \"C:/Users/Lenovo/Downloads/Project/FT_model/FtFullDesc32.bin\"\n",
    "# fasttext_model =\"C:/Users/Lenovo/Downloads/Project/FT_model/cc.en.300.bin\"\n",
    "\n",
    "if(experiment_mode=='train'):\n",
    "    d = \"\"\n",
    "    txt = []\n",
    "\n",
    "    for i in df['DESCRIPTION']:\n",
    "        d =\"\" + i + \".\" \n",
    "        txt.append(d)\n",
    "    #preprocess text is the funcion specified outside the class\n",
    "    final_corpus = [preprocess_text(sentence) for sentence in txt]\n",
    "    word_punctuation_tokenizer = nltk.WordPunctTokenizer()\n",
    "    word_tokenized_corpus = [word_punctuation_tokenizer.tokenize(sent) for sent in final_corpus]\n",
    "    \n",
    "    #embedding_size = 32\n",
    "    #Total unique words: 53983\n",
    "    #originally 60\n",
    "    \n",
    "    window_size = 4\n",
    "    min_word = 5\n",
    "    down_sampling = 1e-2\n",
    "    print(\"FastText model about to start\")\n",
    "    ft_model_desc = FastText(word_tokenized_corpus, vector_size = embedding_size, \n",
    "                            window=window_size, min_count = min_word, \n",
    "                            sample=down_sampling, sg=1, epochs=100)\n",
    "    #FastText_model = \n",
    "\n",
    "    ft_model_desc.save(ft_desc_model)\n",
    "    \n",
    "    \n",
    "ft_model_desc = FastText.load(ft_desc_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6ai8pHC6iTDn",
    "outputId": "3fb8303b-c1e2-44af-c03f-2fdb3370fa07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity: 0.53490126\n"
     ]
    }
   ],
   "source": [
    "word1 = 'radiation'\n",
    "word2 = 'swelling'\n",
    "vector1 = ft_model_comp.wv[word1]\n",
    "vector2 = ft_model_comp.wv[word2]\n",
    "\n",
    "# Normalize vectors\n",
    "normalized_vector1 = vector1 / np.linalg.norm(vector1)\n",
    "normalized_vector2 = vector2 / np.linalg.norm(vector2)\n",
    "\n",
    "# Calculate cosine similarity\n",
    "cosine_similarity = np.dot(normalized_vector1, normalized_vector2)\n",
    "\n",
    "print(\"Cosine similarity:\", cosine_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 653
    },
    "id": "ItGG-yiAtLWb",
    "outputId": "5f672410-0e71-420c-d123-3adb78d23bb0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RCH_PIN</th>\n",
       "      <th>RCH_CVRNO</th>\n",
       "      <th>CREATEDON</th>\n",
       "      <th>SPED_COMLTEXT</th>\n",
       "      <th>SPDD_ICDCODE</th>\n",
       "      <th>DESCRIPTION</th>\n",
       "      <th>SPDD_TYPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>978787</th>\n",
       "      <td>AMC0017937</td>\n",
       "      <td>1380199</td>\n",
       "      <td>2021-01-17</td>\n",
       "      <td>checking</td>\n",
       "      <td>0.0</td>\n",
       "      <td>AJMAN PKG</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1789542</th>\n",
       "      <td>IBS0420875</td>\n",
       "      <td>1907997</td>\n",
       "      <td>2021-10-03</td>\n",
       "      <td>Pain Rt lower chest -;Nondisplaced Fx of dista...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>AJMAN PKG</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2215944</th>\n",
       "      <td>IBS0440034</td>\n",
       "      <td>2142731</td>\n",
       "      <td>2022-01-15</td>\n",
       "      <td>Pain &amp; swelling of left ankle</td>\n",
       "      <td>0.0</td>\n",
       "      <td>AJMAN PKG</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468772</th>\n",
       "      <td>IBS0377205</td>\n",
       "      <td>2255123</td>\n",
       "      <td>2022-03-05</td>\n",
       "      <td>LBA wirh Rt radiation</td>\n",
       "      <td>0.0</td>\n",
       "      <td>AJMAN PKG</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1273476</th>\n",
       "      <td>SCM0047767</td>\n",
       "      <td>2072803</td>\n",
       "      <td>2021-12-18</td>\n",
       "      <td>laser</td>\n",
       "      <td>0.0</td>\n",
       "      <td>AJMAN PKG</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1789564</th>\n",
       "      <td>IBS0421197</td>\n",
       "      <td>1884166</td>\n",
       "      <td>2021-09-23</td>\n",
       "      <td>Dysuria</td>\n",
       "      <td>0.0</td>\n",
       "      <td>AJMAN PKG</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597611</th>\n",
       "      <td>IBS0362029</td>\n",
       "      <td>2338510</td>\n",
       "      <td>2022-04-22</td>\n",
       "      <td>THROAT PAIN BODY PAIN    FEVER   1 DAYS    N...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>AJMAN PKG</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1918240</th>\n",
       "      <td>IBS0380174</td>\n",
       "      <td>2253423</td>\n",
       "      <td>2022-03-03</td>\n",
       "      <td>Dysuria  pain lower abdomen</td>\n",
       "      <td>0.0</td>\n",
       "      <td>AJMAN PKG</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468720</th>\n",
       "      <td>IBS0376276</td>\n",
       "      <td>1469675</td>\n",
       "      <td>2021-03-06</td>\n",
       "      <td>reports.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>AJMAN PKG</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990303</th>\n",
       "      <td>IBS0371812</td>\n",
       "      <td>1034724</td>\n",
       "      <td>2020-06-19</td>\n",
       "      <td>LEAKING</td>\n",
       "      <td>0.0</td>\n",
       "      <td>AJMAN PKG</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2308482</th>\n",
       "      <td>IBS0431358</td>\n",
       "      <td>2342179</td>\n",
       "      <td>2022-04-24</td>\n",
       "      <td>anc</td>\n",
       "      <td>0.0</td>\n",
       "      <td>AJMAN PKG</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1436785</th>\n",
       "      <td>IBS0384669</td>\n",
       "      <td>1763848</td>\n",
       "      <td>2021-07-31</td>\n",
       "      <td>follicular tonsillitis</td>\n",
       "      <td>0.0</td>\n",
       "      <td>AJMAN PKG</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1461824</th>\n",
       "      <td>AMC0013126</td>\n",
       "      <td>1117810</td>\n",
       "      <td>2020-08-11</td>\n",
       "      <td>ANTENTAL CARE OF IUGR</td>\n",
       "      <td>0.0</td>\n",
       "      <td>AJMAN PKG</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990304</th>\n",
       "      <td>IBS0371812</td>\n",
       "      <td>1034645</td>\n",
       "      <td>2020-06-19</td>\n",
       "      <td>LEAKING</td>\n",
       "      <td>0.0</td>\n",
       "      <td>AJMAN PKG</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1726555</th>\n",
       "      <td>FMC0072180</td>\n",
       "      <td>2294713</td>\n",
       "      <td>2022-03-28</td>\n",
       "      <td>followup</td>\n",
       "      <td>0.0</td>\n",
       "      <td>AJMAN PKG</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1789582</th>\n",
       "      <td>IBS0421443</td>\n",
       "      <td>1885704</td>\n",
       "      <td>2021-09-25</td>\n",
       "      <td>fever  cough  running nose - 2 days</td>\n",
       "      <td>0.0</td>\n",
       "      <td>AJMAN PKG</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>787957</th>\n",
       "      <td>SCM0017700</td>\n",
       "      <td>2066647</td>\n",
       "      <td>2021-12-14</td>\n",
       "      <td>laser</td>\n",
       "      <td>0.0</td>\n",
       "      <td>AJMAN PKG</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1600439</th>\n",
       "      <td>IBS0270925</td>\n",
       "      <td>1570770</td>\n",
       "      <td>2021-04-28</td>\n",
       "      <td>missed cycles</td>\n",
       "      <td>0.0</td>\n",
       "      <td>AJMAN PKG</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419112</th>\n",
       "      <td>AMC0007216</td>\n",
       "      <td>1189572</td>\n",
       "      <td>2020-09-23</td>\n",
       "      <td>antenatal care</td>\n",
       "      <td>0.0</td>\n",
       "      <td>AJMAN PKG</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2234699</th>\n",
       "      <td>IBS0447831</td>\n",
       "      <td>2266499</td>\n",
       "      <td>2022-03-11</td>\n",
       "      <td>CHEST PAIN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>AJMAN PKG</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            RCH_PIN  RCH_CVRNO   CREATEDON  \\\n",
       "978787   AMC0017937    1380199  2021-01-17   \n",
       "1789542  IBS0420875    1907997  2021-10-03   \n",
       "2215944  IBS0440034    2142731  2022-01-15   \n",
       "468772   IBS0377205    2255123  2022-03-05   \n",
       "1273476  SCM0047767    2072803  2021-12-18   \n",
       "1789564  IBS0421197    1884166  2021-09-23   \n",
       "597611   IBS0362029    2338510  2022-04-22   \n",
       "1918240  IBS0380174    2253423  2022-03-03   \n",
       "468720   IBS0376276    1469675  2021-03-06   \n",
       "990303   IBS0371812    1034724  2020-06-19   \n",
       "2308482  IBS0431358    2342179  2022-04-24   \n",
       "1436785  IBS0384669    1763848  2021-07-31   \n",
       "1461824  AMC0013126    1117810  2020-08-11   \n",
       "990304   IBS0371812    1034645  2020-06-19   \n",
       "1726555  FMC0072180    2294713  2022-03-28   \n",
       "1789582  IBS0421443    1885704  2021-09-25   \n",
       "787957   SCM0017700    2066647  2021-12-14   \n",
       "1600439  IBS0270925    1570770  2021-04-28   \n",
       "419112   AMC0007216    1189572  2020-09-23   \n",
       "2234699  IBS0447831    2266499  2022-03-11   \n",
       "\n",
       "                                             SPED_COMLTEXT SPDD_ICDCODE  \\\n",
       "978787                                            checking          0.0   \n",
       "1789542  Pain Rt lower chest -;Nondisplaced Fx of dista...          0.0   \n",
       "2215944                     Pain & swelling of left ankle           0.0   \n",
       "468772                               LBA wirh Rt radiation          0.0   \n",
       "1273476                                              laser          0.0   \n",
       "1789564                                           Dysuria           0.0   \n",
       "597611     THROAT PAIN BODY PAIN    FEVER   1 DAYS    N...          0.0   \n",
       "1918240                       Dysuria  pain lower abdomen           0.0   \n",
       "468720                                            reports.          0.0   \n",
       "990303                                             LEAKING          0.0   \n",
       "2308482                                                anc          0.0   \n",
       "1436785                            follicular tonsillitis           0.0   \n",
       "1461824                              ANTENTAL CARE OF IUGR          0.0   \n",
       "990304                                             LEAKING          0.0   \n",
       "1726555                                           followup          0.0   \n",
       "1789582               fever  cough  running nose - 2 days           0.0   \n",
       "787957                                               laser          0.0   \n",
       "1600439                                      missed cycles          0.0   \n",
       "419112                                     antenatal care           0.0   \n",
       "2234699                                         CHEST PAIN          0.0   \n",
       "\n",
       "        DESCRIPTION SPDD_TYPE  \n",
       "978787    AJMAN PKG         P  \n",
       "1789542   AJMAN PKG         F  \n",
       "2215944   AJMAN PKG         F  \n",
       "468772    AJMAN PKG         F  \n",
       "1273476   AJMAN PKG         P  \n",
       "1789564   AJMAN PKG         F  \n",
       "597611    AJMAN PKG         F  \n",
       "1918240   AJMAN PKG         F  \n",
       "468720    AJMAN PKG         F  \n",
       "990303    AJMAN PKG         F  \n",
       "2308482   AJMAN PKG         F  \n",
       "1436785   AJMAN PKG         F  \n",
       "1461824   AJMAN PKG         F  \n",
       "990304    AJMAN PKG         P  \n",
       "1726555   AJMAN PKG         F  \n",
       "1789582   AJMAN PKG         F  \n",
       "787957    AJMAN PKG         P  \n",
       "1600439   AJMAN PKG         F  \n",
       "419112    AJMAN PKG         F  \n",
       "2234699   AJMAN PKG         F  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 215
    },
    "id": "_zq-i_MHUFpp",
    "outputId": "06232da3-3846-470d-f9ea-693ed2deafbe"
   },
   "outputs": [],
   "source": [
    "df_neg.loc[df_neg['SPED_COMLTEXT']=='complaint of pain low back region', ['SPED_COMLTEXT', 'SPDD_ICDCODE', 'DESCRIPTION']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "HIAGIEnN4Djv"
   },
   "outputs": [],
   "source": [
    "#embedding_size = int(input(\"Enter the vector size\"))\n",
    "# Custom data generator : It generates data batch-wise, processes it and sends for training\n",
    "class ICDRetrivalDataGenerator(tf.keras.utils.Sequence):\n",
    "\n",
    "    def __init__(self, df, X_col, y_col, mode,\n",
    "                 batch_size, negative_sample_size, ft_model_comp, ft_model_desc, TRIGRAM_INDICES = None,\n",
    "                 TOTAL_TRIGRAMS = 0, shuffle=True,\n",
    "                 single_query='', unique_document_df=None, embedding_size = embedding_size, df_neg=None, neg_unique_document_df=None): #, ft_model = None\n",
    "        self.df = df.copy()\n",
    "        self.df_neg = df_neg.copy()\n",
    "        # complaint text\n",
    "        self.X_col = X_col\n",
    "        # description\n",
    "        self.y_col = y_col\n",
    "        self.pin_column = 'SPDD_PIN'  # For negative sampling\n",
    "        self.discarded_words = ['undefined', 'unspecified']  # words to be discarded from queries/documents\n",
    "        self.replace_characters = [',', ';', '.', '/', ':', '+', '(', ')', '-', '%', '<', '>', '&', '*', '[', ']',\n",
    "                                    '?', '_']\n",
    "        self.shuffle = shuffle\n",
    "        # one out of 'train' or 'predict_on_single'\n",
    "        self.mode = mode  \n",
    "        self.batch_size = batch_size\n",
    "        self.negative_sample_size = negative_sample_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.ft_model_comp = ft_model_comp\n",
    "        self.ft_model_desc = ft_model_desc\n",
    "        \n",
    "\n",
    "\n",
    "        if (self.mode == 'train'):\n",
    "            #self.create_all_possible_trigrams()\n",
    "            #self.map_unique_pins_to_icds()\n",
    "            # this means that unique_document_df actually is a dataframe consisting of the values of both ICD CODES and their DESCRIPTION \n",
    "            # since they are mapped to each other\n",
    "            self.unique_document_df = self.df[['SPDD_ICDCODE', self.y_col]]\n",
    "            self.unique_document_df = self.unique_document_df.drop_duplicates(subset=['SPDD_ICDCODE'])\n",
    "            #list of all negative unique documents\n",
    "            #self.neg_unique_document_df = self.df_neg[['SPED_COMLTEXT'==self.X_col],['SPDD_ICDCODE', self.y_col]]\n",
    "            self.neg_unique_document_df = df_neg.loc[df_neg['SPED_COMLTEXT']==self.X_col, ['SPDD_ICDCODE', self.y_col]]\n",
    "\n",
    "            \n",
    "            #creating fasttext model while training\n",
    "            \n",
    "            #FastText_model = \n",
    "            \n",
    "            \n",
    "        \n",
    "        if (self.mode == 'predict_on_single'):\n",
    "\n",
    "            #self.total_letter_trigrams = TOTAL_TRIGRAMS\n",
    "            #self.trigrams_to_indices = TRIGRAM_INDICES\n",
    "            self.single_query = single_query\n",
    "            self.unique_document_df = unique_document_df\n",
    "            #self.ft_model = ft_model\n",
    "            #self.fasttext_model = \"C:/Users/Lenovo/Downloads/Project/FT_model/modelFtUn.bin\"\n",
    "            #self.ft_model = FastText.load(self.fasttext_model)\n",
    "\n",
    "    \n",
    "    # calling fasttxt to generate word vector for a nodel which is called in the constructor itself in training mode\n",
    "    def fasttext_word_to_vec(self, word, ft_model):\n",
    "        wordvector = [0] * 32\n",
    "        # get vector for a word\n",
    "        #word = 'example'\n",
    "        try:\n",
    "            wordvector = ft_model.wv[word]\n",
    "        except KeyError:\n",
    "            print(\"missing n-gram\")\n",
    "            pass\n",
    "        # returns word vector on the fly\n",
    "        return wordvector\n",
    "     \n",
    "     \n",
    "\n",
    "\n",
    "    # useful for trigrams\n",
    "    #removed\n",
    "    \n",
    "    # useful for trigrams\n",
    "    # This converts the input words in text form into vectors\n",
    "    # removed\n",
    "\n",
    "    # Function to convert sentence to list of word vectors\n",
    "    def sentence_to_vectors(self, sentence, sentence_type='query'):\n",
    "        #print(sentence)\n",
    "        if (type(sentence) == float):\n",
    "            sentence = \"###\"\n",
    "        # converting sentence to lowercase\n",
    "        sentence = sentence.lower()\n",
    "        # splitting sentenc into words by spaces\n",
    "        words = sentence.split(\" \")\n",
    "        # specifyig the max_length\n",
    "        max_sentence_length = MAX_QUERY_LENGTH\n",
    "        if (sentence_type == 'document'):\n",
    "            # specifying the document length if it's a doc\n",
    "            max_sentence_length = MAX_DOCUMENT_LENGTH\n",
    "        # discarding irrelevant words\n",
    "        words = [x for x in words if x not in self.discarded_words]\n",
    "        # truncating words to the max limit availabe\n",
    "        words = words[:max_sentence_length]\n",
    "        words = words\n",
    "        # initialisig word vectors\n",
    "        word_vectors = []\n",
    "        for word in words:\n",
    "            if word:\n",
    "                for specialChar in self.replace_characters:\n",
    "                  # removinf any special character\n",
    "                    word = word.replace(specialChar, '')\n",
    "                for i in range(10):\n",
    "                    # Replace all digits with ''\n",
    "                    word = word.replace(str(i), '')\n",
    "                # calling the fasttext function on a particular word\n",
    "                if(sentence_type=='document'):\n",
    "                    vec = self.fasttext_word_to_vec(word, self.ft_model_desc)\n",
    "                else:\n",
    "                    vec = self.fasttext_word_to_vec(word, self.ft_model_comp)\n",
    "                # append word vectors\n",
    "                word_vectors.append(vec)\n",
    "            else:\n",
    "                vec = self.fasttext_word_to_vec('#', ft_model_desc)\n",
    "                word_vectors.append(vec)\n",
    "        # padding the sentence if size<max_sentence_length\n",
    "        if (max_sentence_length > len(word_vectors)):\n",
    "            word_vectors = word_vectors + [[0.0] * self.embedding_size for x in\n",
    "                                           range(max_sentence_length - len(word_vectors))]\n",
    "        # Padding in start and end\n",
    "        word_vectors = [[0.0] * self.embedding_size] + word_vectors + [\n",
    "            [0.0] * self.embedding_size] \n",
    "        return word_vectors\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        # reshuffling of rows\n",
    "        self.df = self.df.sample(frac=1)\n",
    "\n",
    "    def __get_data(self, batches):\n",
    "        # Generates data containing batch_size samples\n",
    "\n",
    "        query_vectors = []\n",
    "        document_vectors = []\n",
    "        labels = []\n",
    "\n",
    "        for index, row in batches.iterrows():\n",
    "\n",
    "            if (self.mode == 'train'):\n",
    "\n",
    "                #converting query to sentence and then into vector embeddings\n",
    "                query_vector = self.sentence_to_vectors(row[self.X_col])\n",
    "\n",
    "                # Add positive sample pair\n",
    "                query_vectors.append(query_vector)\n",
    "                #similarly the y values that is the docs\n",
    "                document_vectors.append(self.sentence_to_vectors(row[self.y_col], sentence_type='document'))\n",
    "                # labelling them to 1 as in Positive Document\n",
    "                labels.append(1)\n",
    "                \n",
    "                # taking out data from negative table\n",
    "                negative_document_vectors = self.neg_unique_document_df\n",
    "                negative_samples_population = negative_document_vectors['DESCRIPTION'].tolist()\n",
    "\n",
    "                k = self.negative_sample_size\n",
    "                if len(negative_samples_population) < k:\n",
    "                    k = len(negative_samples_population)\n",
    "                if(k!=0):\n",
    "                  negative_samples = random.sample(negative_document_vectors['DESCRIPTION'].tolist(), k)\n",
    "                  negative_document_vectors = [self.sentence_to_vectors(sample, sentence_type='document') for sample\n",
    "                                                  in negative_samples]\n",
    "                  # Add negative samples pairs\n",
    "                  query_vectors += [query_vector for x in range(k)]\n",
    "                  document_vectors += negative_document_vectors\n",
    "                  # labelling them as 0 coz they are negatve samples\n",
    "                  labels += [0 for x in range(k)]\n",
    "                \"\"\"\n",
    "                # Extract negative samples\n",
    "                pin = row[self.pin_column]\n",
    "                #set of all ICDs which are to be removed\n",
    "                remove_icds = self.id_idx[pin]\n",
    "                #no use\n",
    "                current_icd = row['SPDD_ICDCODE']\n",
    "                if (current_icd not in remove_icds):\n",
    "                    remove_icds.append(current_icd)\n",
    "                \n",
    "                # print(self.unique_document_df['SPDD_ICDCODE'])\n",
    "                #removing all the docs correspondong to the remove_ICDs\n",
    "                available_documents_for_negative_sampling = self.unique_document_df[~self.unique_document_df['SPDD_ICDCODE'].isin(remove_icds)]\n",
    "                #conveting negative sampl space to list\n",
    "                available_documents_for_negative_sampling = available_documents_for_negative_sampling[\n",
    "                    'DESCRIPTION'].tolist()\n",
    "                #no. of ngative sample size \n",
    "                k = self.negative_sample_size\n",
    "                if (k > len(available_documents_for_negative_sampling)):\n",
    "                    k = len(available_documents_for_negative_sampling)\n",
    "                if (k != 0):\n",
    "                    # negatively selecting from sample space\n",
    "                    nagative_samples = random.sample(available_documents_for_negative_sampling, k)\n",
    "                    # conveting them(neg docs) to vectors\n",
    "                    negative_document_vectors = [self.sentence_to_vectors(sample, sentence_type='document') for sample\n",
    "                                                 in nagative_samples]\n",
    "                  \n",
    "                    # Add negative samples pairs\n",
    "                    query_vectors += [query_vector for x in range(k)]\n",
    "                    document_vectors += negative_document_vectors\n",
    "                    # labelling them as 0 coz they are negatve samples\n",
    "                    labels += [0 for x in range(k)]\n",
    "                \"\"\"\n",
    "\n",
    "            elif (self.mode == 'predict_on_single'):\n",
    "                # converting query to vector and appending\n",
    "                try:\n",
    "                    query_vectors.append(self.sentence_to_vectors(self.single_query))\n",
    "                    # converting document  to vector and appending\n",
    "                    document_vectors.append(self.sentence_to_vectors(row[self.y_col], sentence_type='document'))\n",
    "                    # labellig them as positive sammples\n",
    "                    labels.append(1)\n",
    "                except:\n",
    "                    print(\"exception caught\")\n",
    "                    pass\n",
    "\n",
    "            else:\n",
    "                raise (\"Improper mode\")\n",
    "        # converting query vectors to numpy array\n",
    "        try:\n",
    "            query_vectors = np.array(query_vectors)\n",
    "            # for handling the exception where document_vectors is empty\n",
    "            #X_batch = [query_vectors, document_vectors]\n",
    "            # converting document vectors to numpy array\n",
    "            document_vectors = np.array(document_vectors)\n",
    "            # converting labels to numpy array\n",
    "            labels = np.array(labels)\n",
    "            # x contains query and doc\n",
    "            X_batch = [query_vectors, document_vectors]\n",
    "        except:\n",
    "            pass\n",
    "        # y here is labels sent to the __getitem__\n",
    "        return X_batch, labels\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if (self.mode == 'train'):\n",
    "            # takes out the batch to be trained\n",
    "            batches = self.df[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "            #storing the x and y values\n",
    "            X, y = self.__get_data(batches)\n",
    "        elif (self.mode == 'predict_on_single'):\n",
    "            # \n",
    "            self.unique_document_names = self.unique_document_df['SPDD_ICDCODE'].values\n",
    "            # As only unique documents needed for prediction\n",
    "            batches = self.unique_document_df[index * self.batch_size:(index + 1) * self.batch_size]  \n",
    "            X, y = self.__get_data(batches)\n",
    "        else:\n",
    "            raise ('Improper Mode')\n",
    "        return X, y\n",
    "\n",
    "    def __len__(self):\n",
    "        if (self.mode == 'train'):\n",
    "            # takes o the no. of batches to train\n",
    "            return len(self.df) // self.batch_size\n",
    "        elif (self.mode == 'predict_on_single'):\n",
    "            # takes out the no. of batches to test\n",
    "            return math.ceil(len(self.unique_document_df) // self.batch_size) + 1\n",
    "        else:\n",
    "            raise ('Improper Mode')\n",
    "\n",
    "# datagen code removed ->to find out no. of trigrams\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qF8gUqKB4_oi",
    "outputId": "b5deef7b-efd2-446d-e5a8-a76c3ce94344"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 12, 32)]     0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 12, 32)]     0           []                               \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 10, 300)      28800       ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 10, 300)      28800       ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " lambda (Lambda)                (None, 300)          0           ['conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " lambda_1 (Lambda)              (None, 300)          0           ['conv1d_1[0][0]']               \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 128)          38528       ['lambda[0][0]']                 \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 128)          38528       ['lambda_1[0][0]']               \n",
      "                                                                                                  \n",
      " dot (Dot)                      (None, 1)            0           ['dense[0][0]',                  \n",
      "                                                                  'dense_1[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 134,656\n",
      "Trainable params: 134,656\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# CDSSM Build Model\n",
    "\n",
    "query = Input(shape=(MAX_QUERY_LENGTH + 2, embedding_size))\n",
    "doc = Input(shape=(MAX_DOCUMENT_LENGTH + 2, embedding_size))\n",
    "\n",
    "query_conv = Convolution1D(K, FILTER_LENGTH, padding=\"valid\", input_shape=(MAX_QUERY_LENGTH + 2, embedding_size),\n",
    "                           activation=\"tanh\", use_bias=False)(query)  # See equation (2).\n",
    "query_max = Lambda(lambda x: MAX(x, axis=1), output_shape=(K,))(query_conv)\n",
    "query_sem = Dense(L, activation=\"tanh\", input_dim=K)(query_max)  # See section 3.5.\n",
    "\n",
    "doc_conv = Convolution1D(K, FILTER_LENGTH, padding=\"valid\", input_shape=(MAX_DOCUMENT_LENGTH + 2, embedding_size),\n",
    "                         activation=\"tanh\", use_bias=False)(doc)\n",
    "doc_max = Lambda(lambda x: MAX(x, axis=1), output_shape=(K,))(doc_conv)\n",
    "doc_sem = Dense(L, activation=\"tanh\", input_dim=K)(doc_max)\n",
    "\n",
    "cosine_similarities = dot([query_sem, doc_sem], axes=1, normalize=True)  # See equation (4).\n",
    "\n",
    "probs = cosine_similarities  # See equation (5).  \n",
    "\n",
    "model = Model(inputs=[query, doc], outputs=probs)\n",
    "optimizer = keras.optimizers.Adadelta(learning_rate=0.005)\n",
    "model.compile(optimizer = optimizer, loss = \"binary_crossentropy\", metrics=['accuracy'], run_eagerly=True)\n",
    "# model.compile(optimizer=\"adadelta\", loss=\"binary_crossentropy\", metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "cJRLZNWg5E1W"
   },
   "outputs": [],
   "source": [
    "# It takes trained model and input dataframe, and computes top-k documents for each query\n",
    "def fetch_documents_on_queries(model, dataframe, df_neg):\n",
    "    top_k = 10\n",
    "    actual_documents = []\n",
    "    top_predicted_documents = []\n",
    "    #calling class constructor\n",
    "    datagen = ICDRetrivalDataGenerator(dataframe,\n",
    "                                       X_col='SPED_COMLTEXT',\n",
    "                                       y_col='DESCRIPTION',\n",
    "                                       mode='train',\n",
    "                                       batch_size=50,\n",
    "                                       negative_sample_size=4,\n",
    "                                       ft_model_comp = ft_model_comp,\n",
    "                                       ft_model_desc = ft_model_desc,\n",
    "                                       single_query=\"\", embedding_size = embedding_size, df_neg=df_neg)\n",
    "    # unique docs\n",
    "    unique_document_df = datagen.unique_document_df\n",
    "    # defining the fasTtext model\n",
    "    #ft_model = datagen.ft_model\n",
    "    print(\"Total queries to evaluate \", len(dataframe))\n",
    "    # \n",
    "    for index, row in dataframe.iterrows():\n",
    "        # stores the complain text\n",
    "        query = row['SPED_COMLTEXT']\n",
    "        # \n",
    "        datagen_for_query = ICDRetrivalDataGenerator(dataframe,\n",
    "                                                     X_col='SPED_COMLTEXT',\n",
    "                                                     y_col='DESCRIPTION',\n",
    "                                                     mode='predict_on_single',\n",
    "                                                     batch_size=50,\n",
    "                                                     negative_sample_size=4,\n",
    "                                                     ft_model_comp = ft_model_comp,\n",
    "                                                     ft_model_desc = ft_model_desc,\n",
    "                                                     TRIGRAM_INDICES = TRIGRAM_INDICES,\n",
    "                                                     TOTAL_TRIGRAMS = TOTAL_TRIGRAMS,\n",
    "                                                     single_query=query,\n",
    "                                                     unique_document_df=unique_document_df,\n",
    "                                                     embedding_size = embedding_size,\n",
    "                                                     )\n",
    "        try:\n",
    "            prediction_probabilities = model.predict_generator(datagen_for_query, verbose=1)\n",
    "            prediction_probabilities = [item for sublist in prediction_probabilities for item in sublist]\n",
    "            predicted_top_document_indices = np.argsort(-np.array(prediction_probabilities), kind='mergesort')[:top_k]\n",
    "            predicted_documents = [datagen_for_query.unique_document_names[x] for x in predicted_top_document_indices]\n",
    "            actual_documents.append(row['SPDD_ICDCODE'])\n",
    "            top_predicted_documents.append(predicted_documents)\n",
    "            print(\"Processed \", index)\n",
    "        except:\n",
    "            print(\"Exception caught\")\n",
    "            #tensor = tf.convert_to_tensor(top_predicted_documents)\n",
    "            #print(\"Tensor top_predicted_documents shape:\", tensor.shape)\n",
    "            pass\n",
    "        #tensor = tf.convert_to_tensor(top_predicted_documents)\n",
    "        #print(\"Tensor top_predicted_documents shape:\", tensor.shape)\n",
    "    return actual_documents, top_predicted_documents\n",
    "\n",
    "\n",
    "# This function computes top-k accuracy based on parameter 'k'\n",
    "def compute_top_k_accuracy(actual_documents, top_predicted_documents, k):\n",
    "    total = len(actual_documents)\n",
    "    if(total==0):\n",
    "        return 0\n",
    "    correct = 0\n",
    "    for i in range(total):\n",
    "        if (actual_documents[i] in top_predicted_documents[i][:k]):\n",
    "            correct += 1\n",
    "    accuracy = (correct * 1.0) / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "5mlnbt675Kem"
   },
   "outputs": [],
   "source": [
    "if(experiment_mode=='train'):\n",
    "# ratio of train is to test\n",
    "    train_test_split_size = 0.8\n",
    "\n",
    "    # num_samples = df.shape[0]\n",
    "    # df = df.sample(frac=1).reset_index(drop=True)  # randomly shuffle the complete dataframe\n",
    "    # split_sample_numbers = int(num_samples * train_test_split_size)\n",
    "\n",
    "    # train_df = df.iloc[:split_sample_numbers, :]\n",
    "    # test_df = df.iloc[split_sample_numbers:, :]\n",
    "\n",
    "    # splitting the train and test model using statify\n",
    "    train_x, test_x, train_y, test_y = train_test_split(df.iloc[1:,:-1],\n",
    "                                                        df.iloc[1:,-1],\n",
    "                                                        stratify=df.iloc[1:,-1],\n",
    "                                                        train_size=train_test_split_size)\n",
    "    # train_df x and y\n",
    "    train_df = pd.concat([train_x, train_y], axis=1)\n",
    "    # test_df x and y\n",
    "    test_df = pd.concat([test_x, test_y], axis=1)\n",
    "    test_df.to_csv(\"C:/Users/Lenovo/Downloads/Project/Data/TestData32.csv\", index=False)\n",
    "# saving the path of the model where trained data is stored\n",
    "elif(experiment_mode=='test'):\n",
    "    test_df = df\n",
    "save_model_path = \"C:/Users/Lenovo/Downloads/Project/Model/model32Full.h5\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x1y2AZ2p5Q7e",
    "outputId": "f3d58ff8-6dbe-44f4-c271-28fe88e29ee8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "  999/12723 [=>............................] - ETA: 30:09 - loss: 2.0077 - accuracy: 0.8033WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 1999/12723 [===>..........................] - ETA: 27:10 - loss: 1.1065 - accuracy: 0.9017WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 2999/12723 [======>.......................] - ETA: 27:03 - loss: 0.7599 - accuracy: 0.9345WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 3999/12723 [========>.....................] - ETA: 23:40 - loss: 0.5792 - accuracy: 0.9509WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 4999/12723 [==========>...................] - ETA: 20:37 - loss: 0.4684 - accuracy: 0.9607WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 5999/12723 [=============>................] - ETA: 17:46 - loss: 0.3935 - accuracy: 0.9673WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 6999/12723 [===============>..............] - ETA: 14:59 - loss: 0.3394 - accuracy: 0.9719WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 7999/12723 [=================>............] - ETA: 12:16 - loss: 0.2985 - accuracy: 0.9754WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 8999/12723 [====================>.........] - ETA: 9:37 - loss: 0.2665 - accuracy: 0.9782WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 9999/12723 [======================>.......] - ETA: 7:00 - loss: 0.2407 - accuracy: 0.9804WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "10999/12723 [========================>.....] - ETA: 4:25 - loss: 0.2196 - accuracy: 0.9821WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "11999/12723 [===========================>..] - ETA: 1:51 - loss: 0.2019 - accuracy: 0.9836WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "12723/12723 [==============================] - 2266s 178ms/step - loss: 0.1907 - accuracy: 0.9846 - val_loss: 0.0013 - val_accuracy: 1.0000 - lr: 0.0050\n",
      "Epoch 2/20\n",
      "  276/12723 [..............................] - ETA: 30:29 - loss: 0.0062 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 1276/12723 [==>...........................] - ETA: 28:19 - loss: 0.0059 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 2276/12723 [====>.........................] - ETA: 2:38:53 - loss: 0.0056 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 3276/12723 [======>.......................] - ETA: 1:46:29 - loss: 0.0054 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 4276/12723 [=========>....................] - ETA: 1:17:44 - loss: 0.0052 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 5276/12723 [===========>..................] - ETA: 58:35 - loss: 0.0050 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 6276/12723 [=============>................] - ETA: 44:58 - loss: 0.0049 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 7276/12723 [================>.............] - ETA: 34:26 - loss: 0.0047 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 8276/12723 [==================>...........] - ETA: 25:54 - loss: 0.0046 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 9276/12723 [====================>.........] - ETA: 18:44 - loss: 0.0044 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "10276/12723 [=======================>......] - ETA: 12:32 - loss: 0.0043 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "11276/12723 [=========================>....] - ETA: 7:03 - loss: 0.0042 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "12276/12723 [===========================>..] - ETA: 2:05 - loss: 0.0041 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "12723/12723 [==============================] - 3791s 298ms/step - loss: 0.0041 - accuracy: 1.0000 - val_loss: 5.6144e-04 - val_accuracy: 1.0000 - lr: 0.0050\n",
      "Epoch 3/20\n",
      "  553/12723 [>.............................] - ETA: 28:27 - loss: 0.0028 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 1553/12723 [==>...........................] - ETA: 26:09 - loss: 0.0027 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 2553/12723 [=====>........................] - ETA: 23:48 - loss: 0.0027 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 3553/12723 [=======>......................] - ETA: 21:16 - loss: 0.0026 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 4553/12723 [=========>....................] - ETA: 19:07 - loss: 0.0026 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 5553/12723 [============>.................] - ETA: 17:00 - loss: 0.0025 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 6553/12723 [==============>...............] - ETA: 14:41 - loss: 0.0025 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 7553/12723 [================>.............] - ETA: 12:22 - loss: 0.0024 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 8553/12723 [===================>..........] - ETA: 9:55 - loss: 0.0024 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 9553/12723 [=====================>........] - ETA: 7:31 - loss: 0.0023 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "10553/12723 [=======================>......] - ETA: 5:09 - loss: 0.0023 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "11553/12723 [==========================>...] - ETA: 2:46 - loss: 0.0023 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "12553/12723 [============================>.] - ETA: 24s - loss: 0.0022 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "12723/12723 [==============================] - 2106s 166ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 3.5650e-04 - val_accuracy: 1.0000 - lr: 0.0050\n",
      "Epoch 4/20\n",
      "  830/12723 [>.............................] - ETA: 31:35 - loss: 0.0018 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 1830/12723 [===>..........................] - ETA: 28:46 - loss: 0.0017 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 2830/12723 [=====>........................] - ETA: 26:10 - loss: 0.0017 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 3830/12723 [========>.....................] - ETA: 23:19 - loss: 0.0017 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 4830/12723 [==========>...................] - ETA: 20:51 - loss: 0.0017 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 5830/12723 [============>.................] - ETA: 18:11 - loss: 0.0016 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 6830/12723 [===============>..............] - ETA: 15:02 - loss: 0.0016 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 7830/12723 [=================>............] - ETA: 11:55 - loss: 0.0016 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 8830/12723 [===================>..........] - ETA: 9:10 - loss: 0.0016 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 9830/12723 [======================>.......] - ETA: 6:37 - loss: 0.0016 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "10830/12723 [========================>.....] - ETA: 4:48 - loss: 0.0015 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "11830/12723 [==========================>...] - ETA: 2:16 - loss: 0.0015 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "12723/12723 [==============================] - 2099s 165ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 2.5856e-04 - val_accuracy: 1.0000 - lr: 0.0050\n",
      "Epoch 5/20\n",
      "  107/12723 [..............................] - ETA: 21:09 - loss: 0.0013 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 1107/12723 [=>............................] - ETA: 27:27 - loss: 0.0013 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 2107/12723 [===>..........................] - ETA: 22:13 - loss: 0.0013 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 3107/12723 [======>.......................] - ETA: 19:10 - loss: 0.0012 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 4107/12723 [========>.....................] - ETA: 16:52 - loss: 0.0012 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 5107/12723 [===========>..................] - ETA: 14:41 - loss: 0.0012 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 6107/12723 [=============>................] - ETA: 12:40 - loss: 0.0012 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 7107/12723 [===============>..............] - ETA: 10:44 - loss: 0.0012 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 8107/12723 [==================>...........] - ETA: 8:45 - loss: 0.0012 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 9107/12723 [====================>.........] - ETA: 6:50 - loss: 0.0012 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "10107/12723 [======================>.......] - ETA: 4:55 - loss: 0.0012 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "11107/12723 [=========================>....] - ETA: 3:01 - loss: 0.0012 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "12107/12723 [===========================>..] - ETA: 1:09 - loss: 0.0011 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "12723/12723 [==============================] - 3768s 296ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 2.0373e-04 - val_accuracy: 1.0000 - lr: 0.0050\n",
      "Epoch 6/20\n",
      "  384/12723 [..............................] - ETA: 20:43 - loss: 0.0010 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 1384/12723 [==>...........................] - ETA: 19:23 - loss: 9.9890e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 2384/12723 [====>.........................] - ETA: 17:48 - loss: 9.9304e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 3384/12723 [======>.......................] - ETA: 16:16 - loss: 9.8345e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 4384/12723 [=========>....................] - ETA: 14:31 - loss: 9.7558e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 5384/12723 [===========>..................] - ETA: 12:49 - loss: 9.6820e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 6384/12723 [==============>...............] - ETA: 11:04 - loss: 9.6058e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 7384/12723 [================>.............] - ETA: 9:22 - loss: 9.5323e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 8384/12723 [==================>...........] - ETA: 7:38 - loss: 9.4534e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 9384/12723 [=====================>........] - ETA: 5:53 - loss: 9.3955e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "10384/12723 [=======================>......] - ETA: 4:08 - loss: 9.3265e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "11384/12723 [=========================>....] - ETA: 2:22 - loss: 9.2592e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "12384/12723 [============================>.] - ETA: 1:01 - loss: 9.1919e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "12723/12723 [==============================] - 2544s 200ms/step - loss: 9.1669e-04 - accuracy: 1.0000 - val_loss: 1.6844e-04 - val_accuracy: 1.0000 - lr: 0.0050\n",
      "Epoch 7/20\n",
      "  661/12723 [>.............................] - ETA: 21:44 - loss: 8.2869e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 1661/12723 [==>...........................] - ETA: 20:13 - loss: 8.2801e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 2661/12723 [=====>........................] - ETA: 18:09 - loss: 8.1845e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 3661/12723 [=======>......................] - ETA: 16:19 - loss: 8.1392e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 4661/12723 [=========>....................] - ETA: 14:28 - loss: 8.0744e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 5661/12723 [============>.................] - ETA: 12:41 - loss: 8.0323e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 6661/12723 [==============>...............] - ETA: 10:51 - loss: 7.9903e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 7661/12723 [=================>............] - ETA: 9:02 - loss: 7.9375e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 8661/12723 [===================>..........] - ETA: 7:15 - loss: 7.8873e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 9661/12723 [=====================>........] - ETA: 5:28 - loss: 7.8453e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "10661/12723 [========================>.....] - ETA: 3:40 - loss: 7.7948e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "11661/12723 [==========================>...] - ETA: 1:53 - loss: 7.7474e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "12661/12723 [============================>.] - ETA: 6s - loss: 7.7000e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "12723/12723 [==============================] - ETA: 0s - loss: 7.6960e-04 - accuracy: 1.0000\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 4.999999888241291e-06.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12723/12723 [==============================] - 1597s 125ms/step - loss: 7.6960e-04 - accuracy: 1.0000 - val_loss: 1.4364e-04 - val_accuracy: 1.0000 - lr: 0.0050\n",
      "Epoch 8/20\n",
      "  938/12723 [=>............................] - ETA: 21:03 - loss: 7.0762e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 1938/12723 [===>..........................] - ETA: 19:37 - loss: 7.1019e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 2938/12723 [=====>........................] - ETA: 17:37 - loss: 7.1198e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 3938/12723 [========>.....................] - ETA: 15:54 - loss: 7.1132e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 4938/12723 [==========>...................] - ETA: 14:07 - loss: 7.1221e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 5938/12723 [=============>................] - ETA: 12:20 - loss: 7.1159e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 6938/12723 [===============>..............] - ETA: 10:32 - loss: 7.1160e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 7938/12723 [=================>............] - ETA: 8:41 - loss: 7.1143e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 8938/12723 [====================>.........] - ETA: 6:52 - loss: 7.1111e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 9938/12723 [======================>.......] - ETA: 5:03 - loss: 7.1098e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "10938/12723 [========================>.....] - ETA: 3:14 - loss: 7.1093e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "11938/12723 [===========================>..] - ETA: 1:25 - loss: 7.1146e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "12723/12723 [==============================] - 1617s 127ms/step - loss: 7.1126e-04 - accuracy: 1.0000 - val_loss: 1.4364e-04 - val_accuracy: 1.0000 - lr: 5.0000e-06\n",
      "Epoch 9/20\n",
      "  215/12723 [..............................] - ETA: 22:29 - loss: 7.0292e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 1215/12723 [=>............................] - ETA: 20:48 - loss: 7.1117e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 2215/12723 [====>.........................] - ETA: 19:05 - loss: 7.0955e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 3215/12723 [======>.......................] - ETA: 17:18 - loss: 7.0846e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 4215/12723 [========>.....................] - ETA: 15:54 - loss: 7.0760e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 5215/12723 [===========>..................] - ETA: 14:00 - loss: 7.0961e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 6215/12723 [=============>................] - ETA: 12:08 - loss: 7.0950e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 7215/12723 [================>.............] - ETA: 10:19 - loss: 7.0958e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 8215/12723 [==================>...........] - ETA: 8:29 - loss: 7.0995e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 9215/12723 [====================>.........] - ETA: 6:38 - loss: 7.1053e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "10215/12723 [=======================>......] - ETA: 4:45 - loss: 7.0904e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "11215/12723 [=========================>....] - ETA: 2:51 - loss: 7.1020e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "12215/12723 [===========================>..] - ETA: 58s - loss: 7.1055e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "12722/12723 [============================>.] - ETA: 0s - loss: 7.1123e-04 - accuracy: 1.0000\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-09.\n",
      "12723/12723 [==============================] - 1681s 132ms/step - loss: 7.1123e-04 - accuracy: 1.0000 - val_loss: 1.4363e-04 - val_accuracy: 1.0000 - lr: 5.0000e-06\n",
      "Epoch 10/20\n",
      "  492/12723 [>.............................] - ETA: 23:34 - loss: 6.9350e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 1492/12723 [==>...........................] - ETA: 21:44 - loss: 7.0681e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 2492/12723 [====>.........................] - ETA: 19:59 - loss: 7.0466e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 3492/12723 [=======>......................] - ETA: 18:00 - loss: 7.0656e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 4492/12723 [=========>....................] - ETA: 16:01 - loss: 7.0889e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 5492/12723 [===========>..................] - ETA: 14:05 - loss: 7.0857e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 6492/12723 [==============>...............] - ETA: 12:08 - loss: 7.0869e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 7492/12723 [================>.............] - ETA: 10:11 - loss: 7.1011e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 8492/12723 [===================>..........] - ETA: 8:14 - loss: 7.1025e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 9492/12723 [=====================>........] - ETA: 6:17 - loss: 7.1031e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "10492/12723 [=======================>......] - ETA: 4:20 - loss: 7.1009e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "11492/12723 [==========================>...] - ETA: 2:23 - loss: 7.1108e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "12492/12723 [============================>.] - ETA: 26s - loss: 7.1120e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "12723/12723 [==============================] - 1706s 134ms/step - loss: 7.1126e-04 - accuracy: 1.0000 - val_loss: 1.4364e-04 - val_accuracy: 1.0000 - lr: 5.0000e-09\n",
      "Epoch 11/20\n",
      "  769/12723 [>.............................] - ETA: 21:59 - loss: 7.1236e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 1769/12723 [===>..........................] - ETA: 20:28 - loss: 7.1535e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 2769/12723 [=====>........................] - ETA: 18:38 - loss: 7.1248e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 3769/12723 [=======>......................] - ETA: 16:57 - loss: 7.1189e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4769/12723 [==========>...................] - ETA: 14:52 - loss: 7.1278e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 5769/12723 [============>.................] - ETA: 13:00 - loss: 7.1198e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 6769/12723 [==============>...............] - ETA: 11:06 - loss: 7.1176e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 7769/12723 [=================>............] - ETA: 9:13 - loss: 7.1117e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 8769/12723 [===================>..........] - ETA: 7:21 - loss: 7.1021e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 9769/12723 [======================>.......] - ETA: 5:29 - loss: 7.1097e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "10769/12723 [========================>.....] - ETA: 3:37 - loss: 7.1163e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "11769/12723 [==========================>...] - ETA: 1:46 - loss: 7.1154e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "12722/12723 [============================>.] - ETA: 0s - loss: 7.1126e-04 - accuracy: 1.0000\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 4.999999969612645e-12.\n",
      "12723/12723 [==============================] - 1641s 129ms/step - loss: 7.1126e-04 - accuracy: 1.0000 - val_loss: 1.4363e-04 - val_accuracy: 1.0000 - lr: 5.0000e-09\n",
      "Epoch 12/20\n",
      "   46/12723 [..............................] - ETA: 23:35 - loss: 6.7891e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 1046/12723 [=>............................] - ETA: 21:57 - loss: 7.1068e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 2046/12723 [===>..........................] - ETA: 20:11 - loss: 7.1485e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 3046/12723 [======>.......................] - ETA: 18:14 - loss: 7.1299e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 4046/12723 [========>.....................] - ETA: 16:19 - loss: 7.1352e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 5046/12723 [==========>...................] - ETA: 14:27 - loss: 7.1379e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 6046/12723 [=============>................] - ETA: 12:34 - loss: 7.1437e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 7046/12723 [===============>..............] - ETA: 10:41 - loss: 7.1386e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 8046/12723 [=================>............] - ETA: 8:53 - loss: 7.1283e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 9046/12723 [====================>.........] - ETA: 7:05 - loss: 7.1213e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "10046/12723 [======================>.......] - ETA: 5:08 - loss: 7.1133e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "11046/12723 [=========================>....] - ETA: 3:13 - loss: 7.1082e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "12046/12723 [===========================>..] - ETA: 1:18 - loss: 7.1093e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "12723/12723 [==============================] - 1705s 134ms/step - loss: 7.1125e-04 - accuracy: 1.0000 - val_loss: 1.4363e-04 - val_accuracy: 1.0000 - lr: 5.0000e-12\n",
      "Epoch 13/20\n",
      "  323/12723 [..............................] - ETA: 27:02 - loss: 6.9703e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 1323/12723 [==>...........................] - ETA: 22:29 - loss: 7.0530e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 2323/12723 [====>.........................] - ETA: 20:09 - loss: 7.1180e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 3323/12723 [======>.......................] - ETA: 18:07 - loss: 7.1340e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 4323/12723 [=========>....................] - ETA: 16:08 - loss: 7.1252e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 5323/12723 [===========>..................] - ETA: 14:09 - loss: 7.1245e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 6323/12723 [=============>................] - ETA: 12:10 - loss: 7.1211e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 7323/12723 [================>.............] - ETA: 10:15 - loss: 7.1230e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 8323/12723 [==================>...........] - ETA: 8:22 - loss: 7.1219e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 9323/12723 [====================>.........] - ETA: 6:28 - loss: 7.1247e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "10323/12723 [=======================>......] - ETA: 4:34 - loss: 7.1207e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "11323/12723 [=========================>....] - ETA: 2:40 - loss: 7.1171e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "12323/12723 [============================>.] - ETA: 45s - loss: 7.1120e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "12723/12723 [==============================] - ETA: 0s - loss: 7.1125e-04 - accuracy: 1.0000\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 4.999999980020986e-15.\n",
      "12723/12723 [==============================] - 1686s 132ms/step - loss: 7.1125e-04 - accuracy: 1.0000 - val_loss: 1.4364e-04 - val_accuracy: 1.0000 - lr: 5.0000e-12\n",
      "Epoch 14/20\n",
      "  600/12723 [>.............................] - ETA: 21:29 - loss: 7.1518e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 1600/12723 [==>...........................] - ETA: 20:08 - loss: 7.0900e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 2600/12723 [=====>........................] - ETA: 18:14 - loss: 7.1209e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 3600/12723 [=======>......................] - ETA: 16:22 - loss: 7.0927e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 4600/12723 [=========>....................] - ETA: 14:33 - loss: 7.1027e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 5600/12723 [============>.................] - ETA: 13:46 - loss: 7.1019e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 6600/12723 [==============>...............] - ETA: 12:45 - loss: 7.1038e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 7600/12723 [================>.............] - ETA: 11:14 - loss: 7.1132e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 8600/12723 [===================>..........] - ETA: 9:24 - loss: 7.1121e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      " 9600/12723 [=====================>........] - ETA: 7:19 - loss: 7.1092e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "10600/12723 [=======================>......] - ETA: 5:06 - loss: 7.1084e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "11600/12723 [==========================>...] - ETA: 2:45 - loss: 7.1105e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "12600/12723 [============================>.] - ETA: 18s - loss: 7.1114e-04 - accuracy: 1.0000WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "12723/12723 [==============================] - ETA: 0s - loss: 7.1125e-04 - accuracy: 1.0000Restoring model weights from the end of the best epoch: 9.\n",
      "12723/12723 [==============================] - 2268s 178ms/step - loss: 7.1125e-04 - accuracy: 1.0000 - val_loss: 1.4363e-04 - val_accuracy: 1.0000 - lr: 5.0000e-15\n",
      "Epoch 14: early stopping\n",
      "Model Saved\n"
     ]
    }
   ],
   "source": [
    "if(experiment_mode == 'train'):\n",
    "    # no. of rows in the training model\n",
    "    num_samples = train_df.shape[0]\n",
    "    train_validation_split = 0.8\n",
    "    split_sample_numbers = int(num_samples * train_validation_split)\n",
    "    \n",
    "    # 80% data of the training data use for training rest for validation\n",
    "    training_df = train_df.iloc[:split_sample_numbers, :]\n",
    "    validation_df = train_df.iloc[split_sample_numbers:, :]\n",
    "\n",
    "    # EarlyStopping\n",
    "    # stops early if the performance doesn't improve.\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        verbose=1,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    # reducing learing rate when the performance stops improving\n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.001,\n",
    "        patience=2,\n",
    "        verbose=1\n",
    "    )\n",
    "    # saves the model weights during training with the given parameters\n",
    "    model_checkpoint_callback = ModelCheckpoint(\n",
    "        filepath=save_model_path,\n",
    "        monitor='val_loss',\n",
    "        verbose=1,\n",
    "        mode='min',\n",
    "        save_best_only=True)\n",
    "\n",
    "    traingen = ICDRetrivalDataGenerator(training_df, \n",
    "                                        X_col='SPED_COMLTEXT',\n",
    "                                        y_col='DESCRIPTION',\n",
    "                                        mode='train',\n",
    "                                        batch_size=120,\n",
    "                                        negative_sample_size=NEGATIVE_SAMPLE_SIZE, ft_model_comp = ft_model_comp,\n",
    "                                        ft_model_desc = ft_model_desc,embedding_size = embedding_size, df_neg=df_neg)\n",
    "\n",
    "    validgen = ICDRetrivalDataGenerator(validation_df,\n",
    "                                        X_col='SPED_COMLTEXT',\n",
    "                                        y_col='DESCRIPTION',\n",
    "                                        mode='train',\n",
    "                                        batch_size=120,\n",
    "                                        negative_sample_size=NEGATIVE_SAMPLE_SIZE,ft_model_comp = ft_model_comp,\n",
    "                                        ft_model_desc = ft_model_desc, embedding_size = embedding_size, df_neg=df_neg)\n",
    "\n",
    "    history = model.fit_generator(traingen,\n",
    "                                  validation_data=validgen,\n",
    "                                  epochs=5,\n",
    "                                  verbose=1,\n",
    "                                  class_weight = {0:1,1:5},\n",
    "                                  callbacks=[early_stopping, reduce_lr, model_checkpoint_callback])\n",
    "\n",
    "    history.history\n",
    "    # saving weights of the model\n",
    "    model.save_weights(save_model_path)\n",
    "    print(\n",
    "        \"Model Saved\")  # Download model as soon as the training finished. You can find the trained model weights on the left folder pannel\n",
    "    \n",
    "elif (experiment_mode == 'test'):\n",
    "\n",
    "    # Load model \n",
    "    model.load_weights(save_model_path)\n",
    "    print(\"Loaded\")\n",
    "    # reshuffling the rows and adding a new index according to it\n",
    "    test_df = test_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    # test_df = test_df.iloc[:100,:]\n",
    "    #actual_documents, top_predicted_documents = fetch_documents_on_queries(model, test_df)\n",
    "    actual_documents, top_predicted_documents = fetch_documents_on_queries(model, test_df.sample(n=1000), df_neg)\n",
    "\n",
    "    top_3_accuracy = compute_top_k_accuracy(actual_documents, top_predicted_documents, 3)\n",
    "    print(\"Top 3 accuracy \", top_3_accuracy)\n",
    "    top_5_accuracy = compute_top_k_accuracy(actual_documents, top_predicted_documents, 5)\n",
    "    print(\"Top 5 accuracy \", top_5_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rYFz0F6Y7ruZ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
